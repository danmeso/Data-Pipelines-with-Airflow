# -*- coding: utf-8 -*-
"""Data Pipelines with Airflow Meso.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lkD3xAslLYSyMokGXinl3MGV_oB1Fuv8

# **Data Pipelines with Airflow**

**Background Information**

Our telecommunications company, MTN Rwanda, has a vast customer base, and we generate a
large amount of data daily. We must efficiently process and store this data to make informed
business decisions. Therefore, we plan to develop a data pipeline to extract, transform, and load
data from three CSV files and store it in a Postgres database. We require a skilled data
engineer who can use the Airflow tool to develop the pipeline to achieve this.

**Problem Statement**

The main challenge is that the data generated is in a raw format, and we need to process it
efficiently to make it usable for analysis. This requires us to develop a data pipeline that can
extract, transform and load the data from multiple CSV files into a single database, which can
be used for further analysis.
"""

pip install apache-airflow

pip install mysql-connector-python-rf

from google.colab import files
files.upload()

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import pandas as pd
import psycopg2
import logging
import zipfile
import psycopg2
from sqlalchemy import create_engine
import mysql.connector as sql
import pandas as pd

df_customer = pd.read_csv('customer_data.csv')
df_order = pd.read_csv('order_data.csv')
df_payment = pd.read_csv('payment_data.csv')

df_customer.head()

df_order.head()

df_payment.head()

default_args = {
    'owner': 'BUSAZ Telecoms',
    'depends_on_past': False,
    'start_date': datetime(2023, 3, 19),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

# define DAG
dag = DAG(
    'meso_dag',
    default_args=default_args,
    schedule_interval='@daily'
)

# Define Postgres connection details
pg_host = "105.198.57.213"
pg_database = "customer_data"
pg_user = "postgres"
pg_password = "Busaz"

# Define the function to extract the data from the CSV files
def extract_data():
    df_customer = pd.read_csv('customer_data.csv')
    df_order = pd.read_csv('order_data.csv')
    df_payment = pd.read_csv('payment_data.csv')
    return customer_df, order_df, payment_df

def transform_data(): 
    # convert date fields to the correct format using pd.to_datetime
    # merge customer and order dataframes on the customer_id column
    # merge payment dataframe with the merged dataframe on the order_id and customer_id columns
    # drop unnecessary columns like customer_id and order_id
    # group the data by customer and aggregate the amount paid using sum
    # group the data by customer and aggregate the amount paid using sum andcreate a new column to calculate the total value of orders made by each customer
    # calculate the customer lifetime value using the formula CLV = (average order value) x (number of orders made per year) x (average customer lifespan) 
    
    
    # convert date fields to the correct format using pd.to_datetime
    df_customer['date_of_birth'] = pd.to_datetime(df_customer['date_of_birth'])
    df_order['order_date'] = pd.to_datetime(df_order['order_date'])
    df_payment['payment_date'] = pd.to_datetime(df_payment['payment_date'])
     # merge customer and order dataframes on the customer_id column
        
    df_merged = pd.merge(df_customer, df_order, how="right", on=["customer_id"])
    
    # merge payment dataframe with the merged dataframe on the order_id and customer_i"d columns
    df_merged2 = pd.merge(df_merged, df_payment, how="right", on=["customer_id", "order_id"])
    
    # drop unnecessary columns like customer_id and order_id
    df_merged3 =df_merged2.drop(['customer_id', 'order_id'], axis=1)
    
    # group the data by customer and aggregate the amount paid using sum 
    df_transformed = df_merged3.groupby(['first_name', 'last_name',  'country', ])['amount'].sum()

     # Create a new column to calculate the total value of orders made by each customer
    df_merged3['total_order_value'] = df_merged3.groupby(['first_name', 'last_name',  'country', ])['price'].sum().values

# define loading task
def load_data(transformed_records):
    # create PostgresHook
    hook = PostgresHook('my_database')
    # insert transformed records into database
    hook.insert_rows('my_table_transformed', transformed_records)

load_task = PythonOperator(
    task_id='load_data',
    python_callable=load_data,
    op_kwargs={'transformed_records': transform_task.output},
    dag=dag

# set task dependencies
extract_task >> transform_task >> load_task

"""BEST PRACTISES DURING IMPLEMENTATION

Secure connections and credentials: 

Use role-based access control

Secure Airflow web server

Secure Airflow scheduler

Use secure file storage

"""